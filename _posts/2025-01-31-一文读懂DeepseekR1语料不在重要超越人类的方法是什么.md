---
layout: post
title: "一文读懂Deepseek R1，语料不在重要，超越人类的方法是什么？"
subtitle: ""
date: 2025-01-31
author: 
header-img: img/posts/2025/01/investment-bg.jpg
catalog: true
tags:
    - 投资
    - 市场分析
    - 趋势研究
---

🔥🔥🔥春节和家人带娃旅游，到处都是人，还是炒股更轻松。

排队吃饭间隙，看了一篇醍醐灌顶的深度好文。

今天看了一个AI大佬科普DeepSeek的文章，转过来分享一下。OpenAI 的科学家们是最早坚信压缩即智能的那批人，他们认为只要使用更海量优质的数据、在更庞大的 GPU 集群上训练更大参数量的模型，就能产生更大的智能，ChatGPT 就是在这样的信仰之下诞生的。Google 虽然做出了 Transformer，但他们无法进行创业公司那样的豪赌。DeepSeek V3 和 ChatGPT 做的事差不多，因为美国 GPU 出口管制，聪明的研究者被迫使用了更高效的训练技巧(MoE/FP8)，他们也拥有顶尖的基础设施团队，最终只用了 550 万美元就训练了比肩 GPT-4o 的模型，后者的训练成本超过 1 亿美元。但本文重点是 R1。这里想说的是，人类产生的数据在 2024 年底已经被消耗殆尽了，模型的尺寸可以随着 GPU 集群的增加，轻易扩大 10 倍甚至 100 倍，但人类每一年产生的新数据，相比现有的几十年、过去几百年的数据来说，增量几乎可以忽略不计。而按照 Chinchilla 扩展定律（Scaling Laws）：每增加一倍模型大小，训练数据的数量也应增加一倍。这就导致了预训练撞墙的事实：模型体积虽然增加了 10 倍，但我们已经无法获得比现在多 10 倍的高质量数据了。GPT-5 迟迟不发布、国产大模型厂商不做预训练的传闻，都和这个问题有关RLHF 并不是 RL另一方面，基于人类偏好的强化学习(RLHF)最大的问题是：普通人类的智商已经不足以评估模型结果了。在 ChatGPT 时代，AI 的智商低于普通人，所以 OpenAI 可以请大量廉价劳动力，对 AI 的输出结果进行评测：好/中/差，但很快随着 GPT-4o/Claude 3.5 Sonnet 的诞生，大模型的智商已经超越了普通人，只有专家级别的标注人员，才有可能帮助模型提升。且不说聘请专家的成本，那专家之后呢？终究有一天，最顶尖的专家也无法评估模型结果了，AI 就超越人类了吗？并不是。AlphaGo 对李世石下出第 19 手棋，从人类偏好来看，这步棋绝不可能赢，所以如果让李世石来做人类反馈(Human Feedback, HF)评价 AI 的这步棋，他很可能也会给出负分。这样，AI 就永远无法逃出人类思维的枷锁。你可以把 AI 想象成一个学生，给他打分的人从高中老师变成了大学教授，学生的水平会变高，但几乎不可能超越教授。RLHF 本质上是一种讨好人类的训练方式，它让模型输出符合人类偏好，但同时它扼杀了超越人类的可能性。关于 RLHF 和 RL，最近 Andrej Karpathy 也发表了类似的看法[4]:AI 和儿童一样，有两种学习模式。1）通过模仿专家玩家来学习（观察并重复，即预训练，监督微调），2）通过不断试错和强化学习来赢得比赛，我最喜欢的简单例子是 AlphaGo。几乎每一个深度学习的惊人结果，以及所有魔法的来源总是 2。强化学习（RL）很强大，但强化学习与人类反馈（RLHF）并不相同，RLHF 不是 RL。附上我之前的一条想法：
OpenAI 的解法丹尼尔·卡尼曼在《思考快与慢》里提出，人脑对待问题有两种思考模式：一类问题不经过脑子就能给出回答，也就是快思考，一类问题需要类似围棋的长考才能给出答案，也就是慢思考。既然训练已经到头了，那可否从推理，也就是给出回答的时候，通过增加思考时间，从而让回答质量变好呢？这其实也有先例：科学家很早就发现，给模型提问时加一句：“让我们一步一步思考”(“Let’s think step by step”)，可以让模型输出自己的思考过程，最终给出更好的结果，这被称为思维链(Chain-of-Thought, CoT)。2024 年底大模型预训练撞墙后，使用强化学习（RL）来训练模型思维链成为了所有人的新共识。这种训练极大地提高了某些特定、客观可测量任务（如数学、编码）的性能。它需要从普通的预训练模型开始，在第二阶段使用强化学习训练推理思维链，这类模型被称为&nbsp;Reasoning 模型，OpenAI 在 2024 年 9 月发布的 o1 模型以及随后发布的 o3 模型，都是 Reasoning 模型。不同于 ChatGPT 和 GPT-4/4o，在 o1/o3 这类 Reasoning 模型 的训练过程中，人类反馈已经不再重要了，因为可以自动评估每一步的思考结果，从而给予奖励/惩罚。Anthropic 的 CEO 在昨天的文章中[5]用转折点来形容这一技术路线：存在一个强大的新范式，它处于 Scaling Law 的早期，可以快速取得重大进展。虽然 OpenAI 并没有公布他们的强化学习算法细节，但最近 DeepSeek R1 的发布，向我们展示了一种可行的方法。
DeepSeek R1-Zero

我猜 DeepSeek 将自己的纯强化学习模型命名为 R1-Zero 也是在致敬 AlphaZero，那个通过自我对弈、不需要学习任何棋谱就能超越最强棋手的算法。要训练慢思考模型，首先要构造质量足够好的、包含思维过程的数据，并且如果希望强化学习不依赖人类，就需要对思考的每一步进行定量(好/坏)评估，从而给予每一步思考结果奖励/惩罚。正如上文所说：数学和代码这两个数据集最符合要求，数学公式的每一步推导都能被验证是否正确，而代码的输出结果以通过直接在编译器上运行来检验。举个例子，在数学课本中，我们经常看到这样的推理过程：Copy&lt;思考&gt;  设方程根为x, 两边平方得: x² = a - √(a+x)  移项得: √(a+x) = a - x²  再次平方: (a+x) = (a - x²)²  展开: a + x = a² - 2a x² + x⁴  整理: x⁴ - 2a x² - x + (a² - a) = 0&lt;/思考&gt;&lt;回答&gt;x⁴ - 2a x² - x + (a² - a) = 0&lt;/回答&gt;上面这段文本就包含了一个完整的思维链，我们可以通过正则表达式匹配出思考过程和最终回答，从而对模型的每一步推理结果进行定量评估。和 OpenAI 类似，DeepSeek 的研究者基于 V3 模型，在数学和代码这两类包含思维链的数据上进行了强化学习(RL)训练，他们创造了一种名为 GRPO（Group Relative Policy Optimization）的强化学习算法，最终得到的 R1-Zero 模型在各项推理指标上相比 DeepSeek V3 显著提升，证明仅通过 RL 就能激发模型的推理能力。这是另一个 AlphaZero 时刻，在 R1-Zero 的训练过程，完全不依赖人类的智商、经验和偏好，仅靠 RL 去学习那些客观、可测量的人类真理，最终让推理能力远强于所有非 Reasoning 模型。但 R1-Zero 模型只是单纯地进行强化学习，并没有进行监督学习，所以它没有学会人类的问答模式，无法回答人类的问题。并且，它在思考过程中，存在语言混合问题，一会儿说英语、一会儿说中文，可读性差。所以 DeepSeek 团队：先收集了少量高质量的 Chain-of-Thought（CoT）数据，对 V3 模型进行初步的监督微调，解决了输出语言不一致问题，得到冷启动模型。然后，他们在这个冷启动模型上进行类似 R1-Zero 的纯 RL 训练，并加入语言一致性奖励。最后，为了适应更普遍、广泛的非推理任务（如写作、事实问答），他们构造了一组数据对模型进行二次微调。结合推理和通用任务数据，使用混合奖励信号进行最终强化学习。这个过程大概就是：Copy监督学习(SFT) - 强化学习(RL) - 监督学习(SFT) - 强化学习(RL)经过以上过程，就得到了 DeepSeek R1。DeepSeek R1 给世界的贡献是开源世界上第一个比肩闭源(o1)的 Reasoning 模型，现在全世界的用户都可以看到模型在回答问题前的推理过程，也就是&quot;内心独白&quot;，并且完全免费。更重要的是，它向研究者们揭示了 OpenAI 一直在隐藏的秘密：强化学习可以不依赖人类反馈，纯 RL 也能训练出最强的 Reasoning 模型。所以在我心目中，R1-Zero 比 R1 更有意义。对质疑的一些反驳DeepSeek 的 R1 模型，是否真的超越了 OpenAI？从指标上看，R1 的推理能力超越了所有的非 Reasoning 模型，也就是 ChatGPT/GPT-4/4o 和 Claude 3.5 Sonnet，与同为 Reasoning 模型 的 o1接近，逊色于 o3，但 o1/o3 都是闭源模型。很多人的实际体验可能不同，因为 Claude 3.5 Sonnet 在对用户意图理解上更胜一筹。DeepSeek 会收集用户聊天内容用于训练很多人有个误区，认为类似 ChatGPT 这类聊天软件会通过收集用户聊天内容用于训练而变得更聪明，其实不然，如果真是这样，那么微信和 Messenger 就能做出世界上最强的大模型了。相信你看完这篇文章之后就能意识到：大部分普通用户的日常聊天数据已经不重要了。RL 模型只需要在非常高质量的、包含思维链的推理数据上进行训练，例如数学和代码。这些数据可以通过模型自己生成，无需人类标注。因此 做模型数据标注的公司 Scale AI 的 CEO Alexandr Wang 现在很可能正如临大敌，未来的模型对人类标注需求会越来越少。更新：ARC-AGI 的这篇分析 r1-zero 的文章暗示了一个新想法，未来的 Reasoning 模型可以收集用户和模型聊天时 AI 生成的思维链来训练——和人们假想的 AI 偷偷用聊天记录训练不同，用户说了什么其实不重要，在他们付费得到结果的同时，模型 0 成本增加了一条推理思维链数据。DeepSeek R1 厉害是因为偷偷蒸馏了 OpenAI 的模型R1 最主要的性能提升来自强化学习，你可以看到纯 RL、不需要监督数据的 R1-Zero 模型在推理能力上也很强。而 R1 在冷启动时使用了一些监督学习数据，主要是用于解决语言一致性问题，这些数据并不会提升模型的推理能力。另外，很多人对蒸馏有误解：蒸馏通常是指用一个强大的模型作为老师(Teacher)，将它的输出结果用于指导一个参数更小、性能更差的学生(Student)模型，让学生模型直接背答案变得更强，例如 R1 模型可以用于蒸馏 LLama-70B，蒸馏的学生模型性能几乎一定比老师模型更差，但 R1 模型在某些指标性能比 o1 更强，所以说 R1 的性能源于蒸馏 o1 是非常愚蠢的。我问 DeepSeek 它 说自己是 OpenAI 的模型，所以它是套壳的。大模型在训练时并不知道当前的时间，自己究竟被谁训练、训练自己的机器是 H100 还是 H800，X 上有位用户给出了精妙的比喻[8]：这就像你问一个 Uber 乘客，他坐的这辆车轮胎是什么品牌，模型没有理由知道这些信息。一些感受AI 终于除掉了人类反馈的枷锁。DeepSeek R1-Zero 展示了如何使用几乎不使用人类反馈来提升模型性能的方法，这是它的 AlphaZero 时刻。很多人曾说“人工智能，有多少人工就有多少智能”，这个观点可能不再正确了。如果模型能根据直角三角形推导出勾股定理，我们有理由相信它终有一天，能推导出现有数学家尚未发现的定理。写代码是否仍然有意义？我不知道。今早看到 Github 上热门项目 llama.cpp，一个代码共享者提交了 PR，表示他通过对 SIMD 指令加速，将 WASM 运行速度提升 2 倍，而其中 99%的代码由 DeepSeek R1 完成[9]，这肯定不是初级工程师级别的代码了，我无法再说 AI 只能取代初级程序员。ggml : x2 speed for WASM by optimizing SIMD当然，我仍然对此感到非常高兴，人类的能力边界再次被拓展了，干得好 DeepSeek！它是目前世界上最酷的公司。

🔥🔥🔥